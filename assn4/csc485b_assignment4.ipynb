{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dclGnLGAgbtH",
        "outputId": "1ae1397f-10a5-4efe-8744-78a9c28b7cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to c:\\users\\tinyr\\appdata\\local\\temp\\pip-req-build-k_5lp60x\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Source files will be saved in \"C:\\Users\\tinyr\\AppData\\Local\\Temp\\tmp3my80itd\".\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git 'C:\\Users\\tinyr\\AppData\\Local\\Temp\\pip-req-build-k_5lp60x'\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Load the extension that allows us to compile CUDA code in python notebooks\n",
        "# Documentation is here: https://nvcc4jupyter.readthedocs.io/en/latest/\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VVbDQthwogQF"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_types.h\"\n",
        "/**\n",
        " * A collection of commonly used data types throughout this project.\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <iostream> // for std::ostream\n",
        "#include <vector>\n",
        "\n",
        "namespace csc485b{\n",
        "namespace a2{\n",
        "\n",
        "using node_t = int;\n",
        "using edge_t = int2;\n",
        "\n",
        "using edge_list_t = std::vector< edge_t >;\n",
        "using node_list_t = std::vector< node_t >;\n",
        "\n",
        "} // namespace a2\n",
        "} // namespace csc485b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZqET4uI2ggwf"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"cuda_common.h\"\n",
        "/**\n",
        " * Standard macros that can be useful for error checking.\n",
        " * https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <cuda.h>\n",
        "\n",
        "#define CUDA_CALL(exp)                                       \\\n",
        "    do {                                                     \\\n",
        "        cudaError res = (exp);                               \\\n",
        "        if(res != cudaSuccess) {                             \\\n",
        "            printf(\"Error at %s:%d\\n %s\\n\",                  \\\n",
        "                __FILE__,__LINE__, cudaGetErrorString(res)); \\\n",
        "           exit(EXIT_FAILURE);                               \\\n",
        "        }                                                    \\\n",
        "    } while(0)\n",
        "\n",
        "#define CHECK_ERROR(msg)                                             \\\n",
        "    do {                                                             \\\n",
        "        cudaError_t err = cudaGetLastError();                        \\\n",
        "        if(cudaSuccess != err) {                                     \\\n",
        "            printf(\"Error (%s) at %s:%d\\n %s\\n\",                     \\\n",
        "                (msg), __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(EXIT_FAILURE);                                      \\\n",
        "        }                                                            \\\n",
        "    } while (0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "GY0L7rKhoVaZ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_generator.h\"\n",
        "/**\n",
        " * Functions for generating random input data with a fixed seed\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <cassert>  // for assert()\n",
        "#include <cstddef>  // std::size_t type\n",
        "#include <random>   // for std::mt19937, std::uniform_int_distribution\n",
        "#include <vector>\n",
        "\n",
        "#include \"data_types.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a2 {\n",
        "\n",
        "/**\n",
        " * Generates and returns a vector of random edges\n",
        " * for a graph `G=(V,E)` with `n=|V|=n` and expected `m=|E|`.\n",
        " * Referred to as an Erdős-Rényi graph.\n",
        " *\n",
        " * @see https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.fast_gnp_random_graph.html#networkx.generators.random_graphs.fast_gnp_random_graph\n",
        " */\n",
        "edge_list_t generate_graph( std::size_t n, std::size_t m )\n",
        "{\n",
        "    assert( \"At most n(n-1) edges in a simple graph\" && m < n * ( n - 1 ) );\n",
        "\n",
        "    int const probability = ( 100 * m ) / ( n * ( n - 1 ) );\n",
        "\n",
        "    // for details of random number generation, see:\n",
        "    // https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution\n",
        "    std::size_t random_seed = 20241008;  // use magic seed\n",
        "    std::mt19937 rng( random_seed );     // use mersenne twister generator\n",
        "    std::uniform_int_distribution<> distrib(0, 100);\n",
        "\n",
        "    edge_list_t random_edges;\n",
        "    random_edges.reserve( 2 * m );\n",
        "\n",
        "    for( node_t u = 0; u < n; ++u )\n",
        "    {\n",
        "        for( node_t v = u + 1; v < n; ++v )\n",
        "        {\n",
        "            auto const dice_roll = distrib( rng );\n",
        "            if( dice_roll <= probability )\n",
        "            {\n",
        "                random_edges.push_back( make_int2( u, v ) );\n",
        "                random_edges.push_back( make_int2( v, u ) );\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    random_edges.resize( random_edges.size() );\n",
        "\n",
        "\n",
        "    return random_edges;\n",
        "}\n",
        "\n",
        "void generate_matrix( half * matrix, int n, int m ) {\n",
        "    srand(777);\n",
        "\n",
        "    for(int i=0; i<n*m; i++) {\n",
        "        matrix[i] = (float)(rand()) / (float)(rand());\n",
        "        // matrix[i] = 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "} // namespace a2\n",
        "} // namespace csc485b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "bjTbQ3EO2NwQ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"dense_graph.h\"\n",
        "/**\n",
        " * The file in which you will implement your DenseGraph GPU solutions!\n",
        " */\n",
        "\n",
        "#include <cstddef>  // std::size_t type\n",
        "\n",
        "#include \"cuda_common.h\"\n",
        "#include \"data_types.h\"\n",
        "\n",
        "#include <mma.h>\n",
        "using namespace nvcuda;\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a2      {\n",
        "\n",
        "/**\n",
        " * A DenseGraph is optimised for a graph in which the number of edges\n",
        " * is close to n(n-1). It is represented using an adjacency matrix.\n",
        " */\n",
        "struct DenseGraph\n",
        "{\n",
        "    int rows;\n",
        "    int cols;\n",
        "\n",
        "    float* matrix;\n",
        "};\n",
        "\n",
        "\n",
        "namespace gpu {\n",
        "\n",
        "const int WMMA_M = 16;\n",
        "const int WMMA_N = 16;\n",
        "const int WMMA_K = 16;\n",
        "\n",
        "/**\n",
        "  * Repopulates the adjacency matrix as a new graph that represents\n",
        "  * the two-hop neighbourhood of input graph g\n",
        "  */\n",
        "__global__\n",
        "void two_hop_reachability( half * input, float * output, int n, int m )\n",
        "{\n",
        "    /** From tensor core slides **/\n",
        "    int warpM = blockIdx.x;\n",
        "    int warpN = blockIdx.y;\n",
        "\n",
        "    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n",
        "    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b_frag;\n",
        "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;\n",
        "\n",
        "    wmma::fill_fragment(c_frag, 0.0f);\n",
        "\n",
        "    for(int i=0; i < n; i += WMMA_K) {\n",
        "        int aRow = warpM * WMMA_M;\n",
        "        int aCol = i;\n",
        "        int bRow = i;\n",
        "        int bCol = warpN * WMMA_N;\n",
        "\n",
        "        wmma::load_matrix_sync(a_frag, input + (aRow * n + aCol), 16);\n",
        "        wmma::load_matrix_sync(b_frag, input + (bRow * n + bCol), 16);\n",
        "\n",
        "        wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);\n",
        "    }\n",
        "\n",
        "    int out_row = warpM * WMMA_M;\n",
        "    int out_col = warpN * WMMA_N;\n",
        "    wmma::store_matrix_sync(output + (out_row * n + out_col), c_frag, n, wmma::mem_row_major);\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "int run_two_hop( half * input, float * output, int n, int m ) {\n",
        "\n",
        "    dim3 block_dim(32, 1);\n",
        "    dim3 grid_dim(n / WMMA_M, n / WMMA_N);\n",
        "\n",
        "    two_hop_reachability<<< grid_dim, block_dim >>>( input, output, n, m );\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "} // namespace gpu\n",
        "} // namespace a2\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "IRvVeK-QifnZ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"main.cu\"\n",
        "/**\n",
        " * Driver for the benchmark comparison. Generates random data,\n",
        " * runs the CPU baseline, and then runs your code.\n",
        " */\n",
        "\n",
        "#include <chrono>   // for timing\n",
        "#include <iostream> // std::cout, std::endl\n",
        "#include <iterator> // std::ostream_iterator\n",
        "#include <vector>\n",
        "\n",
        "#include \"dense_graph.h\"\n",
        "\n",
        "#include \"data_generator.h\"\n",
        "#include \"data_types.h\"\n",
        "\n",
        "int main()\n",
        "{\n",
        "    using namespace csc485b;\n",
        "\n",
        "    int const n = 16;\n",
        "    int const m = 16;\n",
        "\n",
        "    half matrix[n * m];\n",
        "\n",
        "    a2::generate_matrix(matrix, n, m);\n",
        "\n",
        "    // allocate and memcpy input to device\n",
        "    half * d_matrix;\n",
        "    cudaMalloc( (void**)&d_matrix, sizeof( half ) * n * m );\n",
        "    cudaMemcpyAsync( d_matrix, matrix, sizeof( half ) * n * m, cudaMemcpyHostToDevice );\n",
        "\n",
        "    float * d_result_matrix;\n",
        "    cudaMalloc( (void**)&d_result_matrix, sizeof( float ) * n * m );\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    auto const reachability_start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    int neighbour_size = csc485b::a2::gpu::run_two_hop( d_matrix, d_result_matrix, n, m );\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    auto const end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    float result_matrix[n * m];\n",
        "    cudaMemcpy( result_matrix, d_result_matrix, sizeof( float ) * n * m, cudaMemcpyDeviceToHost );\n",
        "\n",
        "    cudaFree( d_matrix );\n",
        "    cudaFree( d_result_matrix );\n",
        "\n",
        "    for(int i=0; i<n; i++) {\n",
        "        for(int j=0; j<m; j++) {\n",
        "            std::cout << (float)(matrix[(n * i) + j]) << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    for(int i=0; i<n; i++) {\n",
        "        for(int j=0; j<m; j++) {\n",
        "            std::cout << result_matrix[(n * i) + j] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7F0eVsGjUNp",
        "outputId": "9310533d-ca41-46b1-c277-d0df8fb14dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.759277 0.368896 1.80566 0.101562 5.62109 0.744629 2.7832 1.39648 0.263428 1.86719 5.46875 5.70312 3.10156 0.206177 0.700684 1.08203 \n",
            "0.853027 3.83789 0.586426 12.4688 0.327637 0.529785 6.3125 9.98438 0.660156 0.683594 0.778809 11.9609 15.6953 2.39844 1.63574 0.822266 \n",
            "0.0877686 0.0650024 0.577637 1.73633 0.887207 1.12012 4.32422 0.913574 0.890137 0.358887 0.111084 0.563965 11.3281 0.891113 1.44043 10.5625 \n",
            "6.625 2.10156 8.51562 1.84766 2.29883 0.983887 1.35352 0.808594 1.51172 3.44141 0.0393066 0.248657 1.86914 3.35156 10.625 2.05273 \n",
            "0.0176544 0.339355 0.891602 1.09766 0.191406 0.113586 0.411133 1.17773 0.96582 3.92969 0.134521 1.46973 0.76123 0.340576 0.542969 1.6709 \n",
            "2.88867 1.21387 0.454834 0.608887 0.859863 1.00781 4.00391 0.989746 0.286865 1.79688 1.03613 0.847168 1.97656 0.992676 1.40039 1.20215 \n",
            "0.662598 21.2969 1.44141 0.763672 2.02539 0.0375061 1.06641 3.20312 1.04492 0.617676 0.150513 1.47754 2.81055 7.95312 0.355957 0.0563354 \n",
            "0.364502 0.844727 0.637695 1.58496 9.21094 0.773926 0.394531 1.12207 0.496338 2.73633 0.727539 2.49219 0.564453 0.0775757 0.0402832 0.606445 \n",
            "0.582031 11.0234 0.975586 2.29883 0.774414 0.63916 1.12109 0.972656 0.432373 3.79883 0.780273 0.598633 0.257812 1.46484 2.97656 0.032196 \n",
            "0.402588 0.388916 10.6953 7.03125 0.753418 0.905273 1.06445 0.553711 0.893555 6.57422 0.536621 1.14062 5.46875 0.469238 2.18164 2.16797 \n",
            "0.770508 3.88477 0.950684 0.494873 2.22266 0.63623 0.609863 2.5957 0.3396 0.375977 1.71973 0.785156 1.51953 0.559082 0.515625 1.75293 \n",
            "2.54688 0.516602 0.817383 2.10156 1.25977 1.49023 5.95703 2.33008 1.03223 1.52246 0.273438 3.01172 1.78027 1.83789 0.163452 1.0957 \n",
            "0.0199738 0.751953 11.75 4.21094 1.93164 1.4668 7.49219 3.10156 1.00684 4.04688 0.575195 4.5625 0.59668 0.978516 0.729492 0.420898 \n",
            "0.0950928 0.0957031 1117 0.212036 1.08691 1.01953 1.42578 1.2627 4 0.480469 0.300537 0.215942 0.916992 1.00195 0.276855 0.563965 \n",
            "1.56348 0.720215 4.125 1.10645 1.83594 0.467041 1.87988 0.206787 0.691895 1.20703 0.444336 1.17969 1.74023 0.353516 0.27417 3.57812 \n",
            "0.835938 1.08594 0.490234 9.67969 2.95312 0.387695 2.17578 1.53613 0.344482 2.87891 6.77734 1.67773 0.152588 1.15918 0.565918 1.11621 \n",
            "\n",
            "28.0514 97.144 313.999 71.7412 58.088 24.8008 87.9935 64.7754 25.7797 72.2614 29.2691 66.8659 75.3172 47.4326 21.691 56.7252 \n",
            "131.445 215.761 3015.4 201.54 196.686 70.039 258.01 166.683 74.4682 175.312 38.5565 202.587 158.428 146.308 162.61 74.1664 \n",
            "31.7027 130.279 1168.3 168.973 81.9292 28.6513 131.964 75.1832 30.2858 102.121 83.5003 86.9797 41.6996 69.6857 41.5396 37.2745 \n",
            "45.4982 76.3207 3887.83 118.936 98.1925 35.7262 127.762 67.8691 45.1693 93.7411 64.8839 107.249 204.111 48.4053 61.5522 159.063 \n",
            "16.9706 29.8946 448.387 63.3276 28.9708 12.1505 33.9171 20.4504 12.485 49.2971 17.1665 25.4386 46.2502 18.0801 27.4728 27.8797 \n",
            "20.4173 106.875 1178.59 60.8338 52.6598 14.6952 56.9273 48.4648 18.982 47.0626 32.7063 60.389 66.4537 47.3835 22.7052 28.0456 \n",
            "32.026 126.127 8953.56 302.049 64.1838 32.8726 190.842 249.039 58.8303 62.5143 29.5586 293.032 376.598 80.0467 56.3101 50.7133 \n",
            "23.5093 31.9329 151.094 61.5532 30.34 13.8342 43.4924 37.426 20.2993 75.9008 13.9653 45.6611 56.0639 23.4571 34.6836 39.1584 \n",
            "36.85 85.2489 1724.71 181.436 38.2964 19.5331 101.248 129.111 27.0134 57.6836 19.6205 154.117 225.542 51.7849 57.5262 49.3273 \n",
            "63.556 64.7682 741.066 136.99 65.9778 39.7933 130.211 55.6115 40.1466 115.598 29.9803 62.7085 194.17 59.672 116.519 160.165 \n",
            "16.5169 47.4499 665.203 88.4693 48.7254 13.6499 60.0555 64.2952 15.2208 41.1861 27.093 77.6446 89.9243 25.7619 21.184 30.63 \n",
            "35.9692 155.874 2130.21 60.7503 70.4124 20.5115 70.0205 54.9485 28.5847 60.3755 30.8004 61.6006 68.7258 71.8281 40.6209 32.4801 \n",
            "55.8273 195.912 1210.06 99.4545 81.4906 35.9457 118.311 72.1081 43.1019 85.1702 16.3374 63.5746 213.992 102.723 82.8174 159.383 \n",
            "108.286 153.65 1792.42 1968.99 1017.1 1259.9 4854.37 1040.47 1005.95 437.234 135.748 648.905 12669.8 1018.46 1628.5 11807.3 \n",
            "19.9199 63.1921 452.922 77.5266 40.4276 16.1685 64.1457 39.5559 17.5407 48.8518 38.5307 46.555 84.6111 35.7879 30.8819 62.6033 \n",
            "81.6913 107.916 1430.2 80.1681 72.6992 25.0085 53.9625 60.0382 34.327 84.0482 30.2764 49.5025 84.7067 66.0027 121.192 57.2597 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cuda_group_run --group \"source\" --compiler-args \"-O3 -g -std=c++20 -arch=sm_75\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
