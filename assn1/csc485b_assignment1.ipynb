{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11793,"status":"ok","timestamp":1727745425010,"user":{"displayName":"Alden Kuperus","userId":"16872005295733612470"},"user_tz":420},"id":"dclGnLGAgbtH","outputId":"1cee49d7-9cd2-4a2a-d763-0bcc5c5940cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n","  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to c:\\users\\tinyr\\appdata\\local\\temp\\pip-req-build-fu_p04o3\n","  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n","Source files will be saved in \"C:\\Users\\tinyr\\AppData\\Local\\Temp\\tmpeb22z1tg\".\n"]},{"name":"stderr","output_type":"stream","text":["  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git 'C:\\Users\\tinyr\\AppData\\Local\\Temp\\pip-req-build-fu_p04o3'\n","\n","[notice] A new release of pip is available: 24.0 -> 24.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["# Load the extension that allows us to compile CUDA code in python notebooks\n","# Documentation is here: https://nvcc4jupyter.readthedocs.io/en/latest/\n","!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n","%load_ext nvcc4jupyter"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727745425011,"user":{"displayName":"Alden Kuperus","userId":"16872005295733612470"},"user_tz":420},"id":"VVbDQthwogQF"},"outputs":[],"source":["%%cuda_group_save -g \"source\" -n \"data_types.h\"\n","/**\n"," * A collection of commonly used data types throughout this project.\n"," */\n","#pragma once\n","\n","#include <stdint.h> // uint32_t\n","\n","using element_t = uint32_t;"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727745425011,"user":{"displayName":"Alden Kuperus","userId":"16872005295733612470"},"user_tz":420},"id":"ZqET4uI2ggwf"},"outputs":[],"source":["%%cuda_group_save -g \"source\" -n \"cuda_common.h\"\n","/**\n"," * Standard macros that can be useful for error checking.\n"," * https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html\n"," */\n","#pragma once\n","\n","#include <cuda.h>\n","\n","#define CUDA_CALL(exp)                                       \\\n","    do {                                                     \\\n","        cudaError res = (exp);                               \\\n","        if(res != cudaSuccess) {                             \\\n","            printf(\"Error at %s:%d\\n %s\\n\",                  \\\n","                __FILE__,__LINE__, cudaGetErrorString(res)); \\\n","           exit(EXIT_FAILURE);                               \\\n","        }                                                    \\\n","    } while(0)\n","\n","#define CHECK_ERROR(msg)                                             \\\n","    do {                                                             \\\n","        cudaError_t err = cudaGetLastError();                        \\\n","        if(cudaSuccess != err) {                                     \\\n","            printf(\"Error (%s) at %s:%d\\n %s\\n\",                     \\\n","                (msg), __FILE__, __LINE__, cudaGetErrorString(err)); \\\n","            exit(EXIT_FAILURE);                                      \\\n","        }                                                            \\\n","    } while (0)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727745425011,"user":{"displayName":"Alden Kuperus","userId":"16872005295733612470"},"user_tz":420},"id":"GY0L7rKhoVaZ"},"outputs":[],"source":["%%cuda_group_save -g \"source\" -n \"data_generator.h\"\n","/**\n"," * Functions for generating random input data with a fixed seed\n"," */\n","#pragma once\n","\n","#include <random>  // for std::mt19937, std::uniform_int_distribution\n","#include <vector>\n","\n","#include \"data_types.h\"\n","\n","namespace csc485b {\n","namespace a1 {\n","\n","/**\n"," * Generates and returns a vector of random uniform data of a given length, n,\n"," * for any integral type. Input range will be [0, 2n].\n"," */\n","template < typename T >\n","std::vector< T > generate_uniform( std::size_t n )\n","{\n","    // for details of random number generation, see:\n","    // https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution\n","    std::size_t random_seed = 20240916;  // use magic seed\n","    std::mt19937 rng( random_seed );     // use mersenne twister generator\n","    std::uniform_int_distribution<> distrib(0, 2 * n);\n","\n","    std::vector< T > random_data( n ); // init array\n","    std::generate( std::begin( random_data )\n","                 , std::end  ( random_data )\n","                 , [ &rng, &distrib ](){ return static_cast< T >( distrib( rng ) ); });\n","\n","    return random_data;\n","}\n","\n","} // namespace a1\n","} // namespace csc485b"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727745425011,"user":{"displayName":"Alden Kuperus","userId":"16872005295733612470"},"user_tz":420},"id":"IJOKRZuCkDh2"},"outputs":[],"source":["%%cuda_group_save -g \"source\" -n \"algorithm_choices.h\"\n","#pragma once\n","\n","#include <vector>\n","\n","#include \"data_types.h\"\n","\n","namespace csc485b {\n","namespace a1 {\n","namespace cpu {\n","\n","int run_cpu_baseline( std::vector< element_t > data, std::vector< element_t > &output, std::size_t switch_at, std::size_t n );\n","\n","} // namespace cpu\n","\n","\n","namespace gpu {\n","\n","int run_gpu_soln( std::vector< element_t > data, std::vector< element_t > &output, std::size_t switch_at, std::size_t n );\n","\n","} // namespace gpu\n","} // namespace a1\n","} // namespace csc485b"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1727745425011,"user":{"displayName":"Alden Kuperus","userId":"16872005295733612470"},"user_tz":420},"id":"V3lAuiBEhKjc"},"outputs":[],"source":["%%cuda_group_save -g \"source\" -n \"cpu_baseline.cu\"\n","/**\n"," * CPU methods that the GPU should outperform.\n"," */\n","\n","#include \"algorithm_choices.h\"\n","\n","#include <algorithm> // std::sort()\n","#include <chrono>    // for timing\n","#include <iostream>  // std::cout, std::endl\n","\n","namespace csc485b {\n","namespace a1      {\n","namespace cpu     {\n","\n","/**\n"," * Simple solution that just sorts the whole array with a built-in sort\n"," * function and then resorts the last portion in the opposing order with\n"," * a second call to that same built-in sort function.\n"," */\n","void opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n","{\n","    std::sort( data, data + num_elements, std::less< element_t >{} );\n","    std::sort( data + invert_at_pos, data + num_elements, std::greater< element_t >{} );\n","}\n","\n","/**\n"," * Run the single-threaded CPU baseline that students are supposed to outperform\n"," * in order to obtain higher grades on this assignment. Times the execution and\n"," * prints to the standard output (e.g., the screen) that \"wall time.\" Note that\n"," * the functions takes the input by value so as to not perturb the original data\n"," * in place.\n"," */\n","int run_cpu_baseline( std::vector< element_t > data, std::vector< element_t > &output, std::size_t switch_at, std::size_t n )\n","{\n","    auto const cpu_start = std::chrono::high_resolution_clock::now();\n","    opposing_sort( data.data(), switch_at, n );\n","    auto const cpu_end = std::chrono::high_resolution_clock::now();\n","\n","    std::cout << \"CPU Baseline time: \"\n","              << std::chrono::duration_cast<std::chrono::nanoseconds>(cpu_end - cpu_start).count()\n","              << \" ns\" << std::endl;\n","\n","    if(n <= 1 << 10) {\n","        for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n","    }\n","\n","    output = data;\n","\n","    return std::chrono::duration_cast<std::chrono::nanoseconds>(cpu_end - cpu_start).count();\n","}\n","\n","} // namespace cpu\n","} // namespace a1\n","} // namespace csc485b"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1727746249599,"user":{"displayName":"Alden Kuperus","userId":"16872005295733612470"},"user_tz":420},"id":"bjTbQ3EO2NwQ"},"outputs":[],"source":["%%cuda_group_save -g \"source\" -n \"gpu_solution.cu\"\n","/**\n"," * The file in which you will implement your GPU solutions!\n"," */\n","\n","#include \"algorithm_choices.h\"\n","\n","#include <chrono>    // for timing\n","#include <iostream>  // std::cout, std::endl\n","#include \"cuda_common.h\"\n","#include <cooperative_groups.h>\n","namespace cg = cooperative_groups;\n","\n","namespace csc485b {\n","namespace a1      {\n","namespace gpu     {\n","\n","/**\n"," * The CPU baseline benefits from warm caches because the data was generated on\n"," * the CPU. Run the data through the GPU once with some arbitrary logic to\n"," * ensure that the GPU cache is warm too and the comparison is more fair.\n"," */\n","__global__\n","void warm_the_gpu( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n","{\n","    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    // We know this will never be true, because of the data generator logic,\n","    // but I doubt that the compiler will figure it out. Thus every element\n","    // should be read, but none of them should be modified.\n","    if( th_id < num_elements && data[ th_id ] > num_elements * 100 )\n","    {\n","        ++data[ th_id ]; // should not be possible.\n","    }\n","}\n","\n","/**\n"," * Your solution. Should match the CPU output.\n"," */\n","__global__\n","void opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n","{\n","    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if( th_id < num_elements ) {\n","        // Pseudo code from https://en.wikipedia.org/wiki/Bitonic_sorter\n","        for(int outer = 2; outer <= num_elements; outer = outer << 1) {\n","            for(int inner = outer >> 1; inner > 0; inner = inner >> 1) {\n","                int pair = th_id ^ inner;\n","                int dir = th_id & outer;\n","\n","                if(th_id < pair) {\n","                    if ( (dir == 0 && data[th_id] > data[pair]) || (dir != 0 && data[th_id] < data[pair]) ) {\n","                        int temp = data[th_id];\n","                        data[th_id] = data[pair];\n","                        data[pair] = temp;\n","                    }\n","                }\n","\n","                __syncthreads();\n","            }\n","        }\n","        \n","        if( th_id >= 7 * num_elements / 8 ) {\n","          int pair = num_elements + invert_at_pos - th_id - 1;\n","\n","          int temp = data[th_id];\n","          data[th_id] = data[pair];\n","          data[pair] = temp;\n","        }\n","\n","        __syncthreads();\n","\n","        return;\n","    }\n","\n","}\n","\n","__global__\n","void opposing_sort_coop( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n","{\n","    cg::grid_group grid = cg::this_grid();\n","\n","    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if( th_id < num_elements ) {\n","        // Pseudo code from https://en.wikipedia.org/wiki/Bitonic_sorter\n","        for(int outer = 2; outer <= num_elements; outer = outer << 1) {\n","            for(int inner = outer >> 1; inner > 0; inner = inner >> 1) {\n","                int pair = th_id ^ inner;\n","                int dir = th_id & outer;\n","\n","                if(th_id < pair) {\n","                    if ( (dir == 0 && data[th_id] > data[pair]) || (dir != 0 && data[th_id] < data[pair]) ) {\n","                        int temp = data[th_id];\n","                        data[th_id] = data[pair];\n","                        data[pair] = temp;\n","                    }\n","                }\n","\n","                cg::sync(grid);\n","            }\n","        }\n","        \n","        if( th_id >= 7 * num_elements / 8 ) {\n","          int pair = num_elements + invert_at_pos - th_id - 1;\n","\n","          int temp = data[th_id];\n","          data[th_id] = data[pair];\n","          data[pair] = temp;\n","        }\n","\n","        cg::sync(grid);\n","\n","        return;\n","    }\n","\n","}\n","\n","__global__\n","void coop_sort( element_t * data, int start, int end, int direction ) {\n","    cg::grid_group grid = cg::this_grid();\n","\n","    int const th_id = blockIdx.x * blockDim.x + threadIdx.x + start;\n","\n","    if( th_id >= start && th_id < end ) {\n","        // Pseudo code from https://en.wikipedia.org/wiki/Bitonic_sorter\n","        for(int outer = 2; outer <= end - start; outer = outer << 1) {\n","            for(int inner = outer >> 1; inner > 0; inner = inner >> 1) {\n","                int pair = th_id ^ inner;\n","                int dir = (th_id & outer) - (outer * direction);\n","\n","                if(th_id < pair) {\n","                    if ( (dir == 0 && data[th_id] > data[pair]) || (dir != 0 && data[th_id] < data[pair]) ) {\n","                        int temp = data[th_id];\n","                        data[th_id] = data[pair];\n","                        data[pair] = temp;\n","                    }\n","                }\n","\n","                cg::sync(grid);\n","            }\n","        }\n","    }\n","\n","    return;\n","}\n","\n","__global__\n","void bitonic_merge_blocks( element_t * data, int outer, int inner, std::size_t num_elements ) {\n","    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if( th_id < num_elements ) {\n","        int pair = th_id ^ inner;\n","        int dir = th_id & outer;\n","\n","        if(th_id < pair) {\n","            if ( (dir == 0 && data[th_id] > data[pair]) || (dir != 0 && data[th_id] < data[pair]) ) {\n","                int temp = data[th_id];\n","                data[th_id] = data[pair];\n","                data[pair] = temp;\n","            }\n","        }\n","    }\n","\n","    return;\n","}\n","\n","__global__\n","void reverse( element_t * data, std::size_t invert_at_pos, std::size_t num_elements ) {\n","    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if( th_id < num_elements ) {\n","        if( th_id >= 7 * num_elements / 8 ) {\n","            int pair = num_elements + invert_at_pos - th_id - 1;\n","\n","            int temp = data[th_id];\n","            data[th_id] = data[pair];\n","            data[pair] = temp;\n","        }\n","    }\n","    \n","    return;\n","}\n","\n","/**\n"," * Performs all the logic of allocating device vectors and copying host/input\n"," * vectors to the device. Times the opposing_sort() kernel with wall time,\n"," * but excludes set up and tear down costs such as mallocs, frees, and memcpies.\n"," */\n","int run_gpu_soln( std::vector< element_t > data, std::vector< element_t > &output, std::size_t switch_at, std::size_t n )\n","{\n","    // Kernel launch configurations. Feel free to change these.\n","    // This is set to maximise the size of a thread block on a T4, but it hasn't\n","    // been tuned. It's not known if this is optimal.\n","    std::size_t const threads_per_block = 1024;\n","    std::size_t const num_blocks =  ( n + threads_per_block - 1 ) / threads_per_block;\n","\n","    // Allocate arrays on the device/GPU\n","    element_t * d_data;\n","    cudaMalloc( (void**) & d_data, sizeof( element_t ) * n );\n","    CHECK_ERROR(\"Allocating input array on device\");\n","\n","    // Copy the input from the host to the device/GPU\n","    cudaMemcpy( d_data, data.data(), sizeof( element_t ) * n, cudaMemcpyHostToDevice );\n","    CHECK_ERROR(\"Copying input array to device\");\n","\n","    int smemSize = threads_per_block * sizeof(int);\n","\n","    cudaDeviceProp deviceProp;\n","    cudaGetDeviceProperties(&deviceProp, 0);\n","\n","    int processors = deviceProp.multiProcessorCount;\n","    // processors = 2;\n","\n","    // Warm the cache on the GPU for a more fair comparison\n","    warm_the_gpu<<< num_blocks, threads_per_block>>>( d_data, switch_at, n );\n","\n","    // Time the execution of the kernel that you implemented\n","    auto const kernel_start = std::chrono::high_resolution_clock::now();\n","\n","    if( num_blocks == 1) {\n","        opposing_sort<<< num_blocks, threads_per_block >>>( d_data, switch_at, n );\n","\n","    } else if( num_blocks <= processors ) {\n","        void *kernelArgs[] = {\n","            (void *)&d_data, (void *)&switch_at, (void *)&n\n","        };\n","\n","        dim3 dimBlock(threads_per_block, 1, 1);\n","        dim3 dimGrid(num_blocks, 1, 1);\n","\n","        cudaLaunchCooperativeKernel((void *)opposing_sort_coop, dimGrid,\n","                                dimBlock, kernelArgs, smemSize, NULL);\n","\n","    } else {\n","        // Round up to nearest power of two trick from https://graphics.stanford.edu/%7Eseander/bithacks.html#RoundUpPowerOf2\n","        uint32_t divisions = (num_blocks + processors - 1) / processors;\n","        divisions--;\n","        divisions |= divisions >> 1;\n","        divisions |= divisions >> 2;\n","        divisions |= divisions >> 4;\n","        divisions |= divisions >> 8;\n","        divisions |= divisions >> 16;\n","        divisions++;\n","\n","        int total_threads = threads_per_block * num_blocks;\n","\n","        // Create sorted subarrays\n","        int direction = 0;\n","        for(int i = 0; i < divisions; i++) {\n","            int start = (total_threads / divisions) * i;\n","            int end = start + (total_threads / divisions);\n","\n","            void *kernelArgs[] = {\n","                (void *)&d_data, (void *)&start, (void *)&end, (void *)&direction\n","            };\n","\n","            dim3 dimBlock(threads_per_block, 1, 1);\n","            dim3 dimGrid(num_blocks / divisions, 1, 1);\n","\n","            cudaLaunchCooperativeKernel((void *)coop_sort, dimGrid,\n","                                    dimBlock, kernelArgs, smemSize, NULL);\n","        }\n","\n","        // Merge/sort the subarrays\n","        for(int outer = 2 * n / divisions; outer < n; outer = outer << 1) {\n","            for(int inner = outer >> 1; inner > 0; inner = inner >> 1) {\n","                bitonic_merge_blocks<<< num_blocks, threads_per_block >>>( d_data, outer, inner, n );\n","            }\n","        }\n","        \n","        for(int inner = n >> 1; inner >= total_threads / divisions; inner = inner >> 1) {\n","            bitonic_merge_blocks<<< num_blocks, threads_per_block >>>( d_data, n, inner, n );\n","        }\n","        \n","        // Sort again\n","        direction = 0;\n","        for(int i = 0; i < divisions; i++) {\n","            int start = (total_threads / divisions) * i;\n","            int end = start + (total_threads / divisions);            \n","\n","            void *kernelArgs[] = {\n","                (void *)&d_data, (void *)&start, (void *)&end, (void *)&direction\n","            };\n","\n","            dim3 dimBlock(threads_per_block, 1, 1);\n","            dim3 dimGrid(num_blocks / divisions, 1, 1);\n","\n","            cudaLaunchCooperativeKernel((void *)coop_sort, dimGrid,\n","                                    dimBlock, kernelArgs, smemSize, NULL);\n","            \n","            direction = 1 - direction;\n","        }\n","        \n","        reverse<<< num_blocks, threads_per_block >>>( d_data, switch_at, n );\n","    }\n","\n","    auto const kernel_end = std::chrono::high_resolution_clock::now();\n","    CHECK_ERROR(\"Executing kernel on device\");\n","\n","    // After the timer ends, copy the result back, free the device vector,\n","    // and echo out the timings and the results.\n","    cudaMemcpy( data.data(), d_data, sizeof( element_t ) * n, cudaMemcpyDeviceToHost );\n","    CHECK_ERROR(\"Transferring result back to host\");\n","    cudaFree( d_data );\n","    CHECK_ERROR(\"Freeing device memory\");\n","\n","    std::cout << \"GPU Solution time: \"\n","              << std::chrono::duration_cast<std::chrono::nanoseconds>(kernel_end - kernel_start).count()\n","              << \" ns\" << std::endl;\n","\n","    if(n <= 1 << 10) {\n","        for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n","    }\n","    \n","    output = data;\n","\n","    return std::chrono::duration_cast<std::chrono::nanoseconds>(kernel_end - kernel_start).count();\n","}\n","\n","} // namespace gpu\n","} // namespace a1\n","} // namespace csc485b"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1727746249811,"user":{"displayName":"Alden Kuperus","userId":"16872005295733612470"},"user_tz":420},"id":"IRvVeK-QifnZ"},"outputs":[],"source":["%%cuda_group_save -g \"source\" -n \"main.cu\"\n","/**\n"," * Driver for the benchmark comparison. Generates random data,\n"," * runs the CPU baseline, and then runs your code.\n"," */\n","\n","#include <cstddef>  // std::size_t type\n","#include <iostream> // std::cout, std::endl\n","#include <vector>\n","\n","#include \"algorithm_choices.h\"\n","#include \"data_generator.h\"\n","#include \"data_types.h\"\n","#include \"cuda_common.h\"\n","\n","int main()\n","{\n","    std::size_t const n =  1 << 20;\n","    std::size_t const switch_at = 3 * ( n >> 2 ) ;\n","\n","    auto data = csc485b::a1::generate_uniform< element_t >( n );\n","\n","    std::vector<element_t> gpu_output;\n","    std::vector<element_t> cpu_output;\n","\n","    std::cout << \"List size: \" << n << std::endl;\n","\n","    int cpu_time = csc485b::a1::cpu::run_cpu_baseline( data, cpu_output, switch_at, n );\n","    int gpu_time = csc485b::a1::gpu::run_gpu_soln( data, gpu_output, switch_at, n );\n","\n","    if(cpu_output == gpu_output) {\n","        std::cout << \"Arrays match!\" << std::endl;\n","    } else {\n","        std::cout << \"Arrays DO NOT match!\" << std::endl;\n","\n","        for(int i=0; i<cpu_output.size(); i++) {\n","            if(cpu_output[i] != gpu_output[i]) {\n","                std::cout << \"First mismatch at index \" << i << \"  \" << cpu_output[i] << std::endl;\n","                \n","                break;\n","            }\n","        }\n","    }\n","\n","    return EXIT_SUCCESS;\n","}"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12267,"status":"ok","timestamp":1727746262263,"user":{"displayName":"Alden Kuperus","userId":"16872005295733612470"},"user_tz":420},"id":"S7F0eVsGjUNp","outputId":"6ba4d627-93f3-4334-99df-006ae552cdbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["List size: 1048576\n","CPU Baseline time: 61850000 ns\n","GPU Solution time: 309500 ns\n","Arrays match!\n","\n"]}],"source":["%cuda_group_run --group \"source\" --compiler-args \"-O3 -g -std=c++20 -arch=sm_75\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"}},"nbformat":4,"nbformat_minor":0}
